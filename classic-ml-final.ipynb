{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Imports and Prerequisites"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:42:50.888799Z","iopub.status.busy":"2024-02-20T08:42:50.888354Z","iopub.status.idle":"2024-02-20T08:42:50.895639Z","shell.execute_reply":"2024-02-20T08:42:50.894141Z","shell.execute_reply.started":"2024-02-20T08:42:50.888769Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import seaborn as sns\n","from tqdm import tqdm \n","# from sklearn.preprocessing import LabelEncoder\n","# from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier"]},{"cell_type":"markdown","metadata":{},"source":["# Kaggle Specific Working Directory"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:42:50.897986Z","iopub.status.busy":"2024-02-20T08:42:50.897551Z","iopub.status.idle":"2024-02-20T08:42:55.376243Z","shell.execute_reply":"2024-02-20T08:42:55.374829Z","shell.execute_reply.started":"2024-02-20T08:42:50.897958Z"},"trusted":true},"outputs":[],"source":["# delete all the files and directories recursively in the current working directory ...\n","\n","!rm -rf *\n","\n","# make directory ...\n","\n","!mkdir /kaggle/working/datasets\n","!mkdir /kaggle/working/datasets/train\n","!mkdir /kaggle/working/datasets/test\n","\n","#  reference original files without duplicating their content ...\n","\n","def all_files_in_folder_symlink(source_dir, target_dir):\n","    files = os.listdir(source_dir)\n","    \n","    for file in tqdm(files):\n","        source_file = os.path.join(source_dir, file)\n","        target_file = os.path.join(target_dir, file)\n","        os.symlink(source_file, target_file)\n","# symbolic link function as above ...\n","\n","\n","all_files_in_folder_symlink(\"/kaggle/input/biomed-datathon-bmefest2/train\",\"/kaggle/working/datasets/train\")\n","all_files_in_folder_symlink(\"/kaggle/input/biomed-datathon-bmefest2/test\",\"/kaggle/working/datasets/test\")\n","os.symlink(\"/kaggle/input/biomed-datathon-bmefest2/additional_metadata.csv\", \"/kaggle/working/datasets/additional_metadata.csv\")\n","os.symlink(\"/kaggle/input/biomed-datathon-bmefest2/sample_submission.csv\", \"/kaggle/working/datasets/sample_submission.csv\")\n","os.symlink(\"/kaggle/input/biomed-datathon-bmefest2/test_files.csv\", \"/kaggle/working/datasets/test_files.csv\")\n","os.symlink(\"/kaggle/input/biomed-datathon-bmefest2/train.csv\", \"/kaggle/working/datasets/train.csv\")\n","\n","os.symlink (\"/kaggle/input/biomed-datathon-bmefest2/train/085_sit_Tri6_06.wav\", \"/kaggle/working/datasets/train/085_sit_Tri.wav\")\n","\n","train_dir_path = \"/kaggle/working/datasets/train/\"\n","test_dir_path = \"/kaggle/working/datasets/test/\""]},{"cell_type":"markdown","metadata":{},"source":["# Loading Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:42:55.379699Z","iopub.status.busy":"2024-02-20T08:42:55.379009Z","iopub.status.idle":"2024-02-20T08:42:55.414666Z","shell.execute_reply":"2024-02-20T08:42:55.413486Z","shell.execute_reply.started":"2024-02-20T08:42:55.379655Z"},"trusted":true},"outputs":[],"source":["# train_dir_path = \"train/\"\n","# test_dir_path = \"test/\"\n","\n","# load the train, test, sample csv [reset]\n","train_df = pd.read_csv(\"/kaggle/working/datasets/train.csv\")\n","test_df = pd.read_csv(\"/kaggle/working/datasets/test_files.csv\")\n","# sample_submit_df = pd.read_csv(\"sample_submission.csv\")\n","# check the additional metadata\n","# metadata = pd.read_csv(\"additional_metadata.csv\")\n","\n","# print the shape\n","print(train_df.info())\n","print(test_df.info())\n","# metadata.info()"]},{"cell_type":"markdown","metadata":{},"source":["# Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:42:55.416985Z","iopub.status.busy":"2024-02-20T08:42:55.416267Z","iopub.status.idle":"2024-02-20T08:43:07.675637Z","shell.execute_reply":"2024-02-20T08:43:07.674404Z","shell.execute_reply.started":"2024-02-20T08:42:55.416941Z"},"trusted":true},"outputs":[],"source":["# Librosa (the mother of audio files)\n","import librosa\n","import librosa.display\n","import IPython.display as ipd\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# import a sample audio file\n","audio_file = train_dir_path + \"002_sit_Aor.wav\"\n","aud, sr = librosa.load(audio_file, sr=None)\n","print(f\"shape of the audio file: {aud.shape}\")\n","print(f\"sample rate: {sr}\")\n","\n","# length of the audio file\n","print(f\"length of the audio file: {len(aud)/sr} seconds\")"]},{"cell_type":"markdown","metadata":{},"source":["# Segmentation / Cutting"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:43:07.679658Z","iopub.status.busy":"2024-02-20T08:43:07.678872Z","iopub.status.idle":"2024-02-20T08:43:07.687974Z","shell.execute_reply":"2024-02-20T08:43:07.686780Z","shell.execute_reply.started":"2024-02-20T08:43:07.679622Z"},"trusted":true},"outputs":[],"source":["# each audio file is 20 seconds long\n","# cut each audio file into 4 parts of 5 seconds each\n","# then add those 4 parts to the dataframe, instead of the original audio file\n","\n","# create a function to cut the audio file into 4 parts\n","def segment_audio_files(file, n_segments=4, segment_length=5):\n","    # load the audio file\n","    aud, sr = librosa.load(file, sr=None)\n","    # get the length of the audio file\n","    length = len(aud)\n","    # get the length of each segment\n","    segment_length = sr * segment_length\n","    # get the number of samples per segment\n","    samples_per_segment = length // n_segments\n","    # get the starting point of each segment\n","    start = 0\n","    # get the ending point of each segment\n","    end = samples_per_segment\n","    # create an empty list to store the segments\n","    segments = []\n","    # loop through the audio file and get the segments\n","    for _ in range(n_segments):\n","        segment = aud[start:end]\n","        segments.append(segment)\n","        start = end\n","        end = start + samples_per_segment\n","    return segments\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:43:07.689942Z","iopub.status.busy":"2024-02-20T08:43:07.689556Z","iopub.status.idle":"2024-02-20T08:43:07.704567Z","shell.execute_reply":"2024-02-20T08:43:07.703428Z","shell.execute_reply.started":"2024-02-20T08:43:07.689906Z"},"trusted":true},"outputs":[],"source":["# # test the function\n","segments = segment_audio_files(audio_file)\n","# print the length of the segments and sr\n","for segment in segments:\n","    print(f\"length of the segment: {len(segment)/sr} seconds\")\n","    print(f\"sample rate: {sr}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:43:07.706504Z","iopub.status.busy":"2024-02-20T08:43:07.706048Z","iopub.status.idle":"2024-02-20T08:43:07.712299Z","shell.execute_reply":"2024-02-20T08:43:07.711195Z","shell.execute_reply.started":"2024-02-20T08:43:07.706466Z"},"trusted":true},"outputs":[],"source":["# # plot the original audio file\n","# plt.figure(figsize=(14, 5))\n","# librosa.display.waveshow(aud, sr=sr, color=\"green\")\n","# plt.title(\"Original Audio File\")\n","# # plt.show()\n","\n","# # plot the segments\n","# plt.figure(figsize=(14, 5))\n","# for i, segment in enumerate(segments):\n","#     plt.subplot(4, 1, i+1)\n","#     librosa.display.waveshow(segment, sr=sr, color=\"green\")\n","#     plt.title(f\"Segment {i+1}\")\n","\n","# plt.tight_layout()\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:43:07.714088Z","iopub.status.busy":"2024-02-20T08:43:07.713769Z","iopub.status.idle":"2024-02-20T08:43:14.688464Z","shell.execute_reply":"2024-02-20T08:43:14.687376Z","shell.execute_reply.started":"2024-02-20T08:43:07.714061Z"},"trusted":true},"outputs":[],"source":["# convert all train and test audio files into segments\n","record_classes = [\"recording_1\", \"recording_2\", \"recording_3\", \"recording_4\", \"recording_5\", \"recording_6\", \"recording_7\", \"recording_8\"]\n","labels = [\"AS\", \"AR\", \"MR\", \"MS\", \"N\"]\n","\n","# create a new dataframe that has the same headers as the train dataframe\n","new_train_df = pd.DataFrame(columns=train_df.columns)\n","# print(new_train_df)\n","temp = {}\n","\n","# process each row in the train dataframe\n","for i, row in train_df.iterrows():\n","    # extract the labels first \n","    label = train_df.iloc[i][1:6] \n","    # print(label)\n","    # get the audio file from each of the 8 columns\n","    for col in record_classes:\n","        audio_file = train_dir_path + row[col] + \".wav\"\n","        segments = segment_audio_files(audio_file)\n","        # now we have 4 segments all of which has the similar labels\n","        temp[col] = segments\n","    # print(len(temp))\n","    # now we have 8 segments for each row\n","    # we need to add each segment to the new dataframe\n","    for i in range(4):\n","        new_row = row.copy()\n","        for col in record_classes:\n","            new_row[col] = temp[col][i]\n","        new_train_df = new_train_df._append(new_row, ignore_index=True)\n","train_df = new_train_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:43:14.690016Z","iopub.status.busy":"2024-02-20T08:43:14.689715Z","iopub.status.idle":"2024-02-20T08:43:14.833449Z","shell.execute_reply":"2024-02-20T08:43:14.832318Z","shell.execute_reply.started":"2024-02-20T08:43:14.689991Z"},"trusted":true},"outputs":[],"source":["train_df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:43:14.834997Z","iopub.status.busy":"2024-02-20T08:43:14.834705Z","iopub.status.idle":"2024-02-20T08:43:14.840800Z","shell.execute_reply":"2024-02-20T08:43:14.839299Z","shell.execute_reply.started":"2024-02-20T08:43:14.834973Z"},"trusted":true},"outputs":[],"source":["# # do the same for test dataframe\n","# new_test_df = pd.DataFrame(columns=test_df.columns)\n","# temp = {}\n","\n","# # process each row in the test dataframe\n","# for i, row in test_df.iterrows():\n","#     # get the audio file from each of the 8 columns\n","#     for col in record_classes:\n","#         audio_file = test_dir_path + row[col] + \".wav\"\n","#         segments = segment_audio_files(audio_file)\n","#         # now we have 4 segments all of which has the similar labels\n","#         temp[col] = segments\n","#     # now we have 8 segments for each row\n","#     # we need to add each segment to the new dataframe\n","#     for i in range(4):\n","#         new_row = row.copy()\n","#         for col in record_classes:\n","#             new_row[col] = temp[col][i]\n","#         new_test_df = new_test_df._append(new_row, ignore_index=True)\n","# test_df = new_test_df"]},{"cell_type":"markdown","metadata":{},"source":["# Feature Extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:43:14.845757Z","iopub.status.busy":"2024-02-20T08:43:14.845285Z","iopub.status.idle":"2024-02-20T08:43:28.092675Z","shell.execute_reply":"2024-02-20T08:43:28.091480Z","shell.execute_reply.started":"2024-02-20T08:43:14.845709Z"},"trusted":true},"outputs":[],"source":["# 1. zero crossing rate column for all the audio files\n","zero_crossing_rate_list = []\n","# create slot for 8 plots in a single figure\n","plt.figure(figsize=(15, 10))\n","for i in range(8):\n","    column_name = f\"recording_{i+1}\"\n","    new_column_name = f\"zero_crossing_rate_{i+1}\"\n","    zero_crossing_rate = []\n","    for aud in train_df[column_name]:\n","        # file_path = train_dir_path + audio_file + \".wav\"\n","        # aud, sr = librosa.load(file_path, sr=None)\n","        zero_crossing_rate.append(librosa.feature.zero_crossing_rate(np.array(aud), frame_length=1024, hop_length=256).mean())\n","    zero_crossing_rate_list.append(zero_crossing_rate)\n","    train_df[new_column_name] = zero_crossing_rate\n","    # plot the newly added column and put all the plots in a single figure\n","    # keep some gaps between the plots\n","    plt.subplot(4, 2, i+1)\n","    plt.title(new_column_name)\n","    plt.plot(zero_crossing_rate)\n","    plt.tight_layout()\n","plt.show()\n","\n","plt.figure(figsize=(15, 10))\n","zero_crossing_rate_list = []\n","for i in range(8):\n","    column_name = f\"recording_{i+1}\"\n","    new_column_name = f\"zero_crossing_rate_{i+1}\"\n","    zero_crossing_rate = []\n","    for audio_file in test_df[column_name]:\n","        file_path = test_dir_path + audio_file + \".wav\"\n","        aud, sr = librosa.load(file_path, sr=None)\n","        zero_crossing_rate.append(librosa.feature.zero_crossing_rate(aud, frame_length=1024, hop_length=256).mean())\n","    zero_crossing_rate_list.append(zero_crossing_rate)\n","    test_df[new_column_name] = zero_crossing_rate\n","    plt.subplot(4, 2, i+1)\n","    plt.title(new_column_name)\n","    plt.plot(zero_crossing_rate)\n","    plt.tight_layout()\n","plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:43:28.094943Z","iopub.status.busy":"2024-02-20T08:43:28.094516Z","iopub.status.idle":"2024-02-20T08:43:33.926535Z","shell.execute_reply":"2024-02-20T08:43:33.925411Z","shell.execute_reply.started":"2024-02-20T08:43:28.094907Z"},"trusted":true},"outputs":[],"source":["# 2. energy column for all the audio files\n","energy_list = []\n","plt.figure(figsize=(15, 10))\n","for i in range(8):\n","    column_name = f\"recording_{i+1}\"\n","    new_column_name = f\"energy_{i+1}\"\n","    energy = []\n","    for aud in train_df[column_name]:\n","        energy.append((np.sum(aud**2))/len(aud))\n","    energy_list.append(energy)\n","    train_df[new_column_name] = energy\n","    plt.subplot(4, 2, i+1)\n","    plt.title(new_column_name)\n","    plt.plot(energy)\n","    plt.tight_layout()\n","\n","plt.show()\n","\n","energy_list = []\n","plt.figure(figsize=(15, 10))\n","for i in range(8):\n","    column_name = f\"recording_{i+1}\"\n","    new_column_name = f\"energy_{i+1}\"\n","    energy = []\n","    for audio_file in test_df[column_name]:\n","        file_path = test_dir_path + audio_file + \".wav\"\n","        aud, sr = librosa.load(file_path, sr=None)\n","        energy.append((np.sum(aud**2))/len(aud))\n","    energy_list.append(energy)\n","    test_df[new_column_name] = energy\n","    plt.subplot(4, 2, i+1)\n","    plt.title(new_column_name)\n","    plt.plot(energy)\n","    plt.tight_layout()\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:43:33.928925Z","iopub.status.busy":"2024-02-20T08:43:33.928206Z","iopub.status.idle":"2024-02-20T08:43:39.503692Z","shell.execute_reply":"2024-02-20T08:43:39.502855Z","shell.execute_reply.started":"2024-02-20T08:43:33.928883Z"},"trusted":true},"outputs":[],"source":["# 3. amplitude for all the audio files\n","amplitude_list = []\n","plt.figure(figsize=(15, 10))\n","for i in range(8):\n","    column_name = f\"recording_{i+1}\"\n","    new_column_name = f\"amplitude_{i+1}\"\n","    amplitude = []\n","    for aud in train_df[column_name]:\n","        amplitude.append(np.max(np.abs(aud)))\n","    amplitude_list.append(amplitude)\n","    train_df[new_column_name] = amplitude\n","    plt.subplot(4, 2, i+1)\n","    plt.title(new_column_name)\n","    plt.plot(amplitude)\n","    plt.tight_layout()\n","\n","plt.show()\n","\n","amplitude_list = []\n","plt.figure(figsize=(15, 10))\n","for i in range(8):\n","    column_name = f\"recording_{i+1}\"\n","    new_column_name = f\"amplitude_{i+1}\"\n","    amplitude = []\n","    for audio_file in test_df[column_name]:\n","        file_path = test_dir_path + audio_file + \".wav\"\n","        aud, sr = librosa.load(file_path, sr=None)\n","        amplitude.append(np.max(np.abs(aud)))\n","    amplitude_list.append(amplitude)\n","    test_df[new_column_name] = amplitude\n","    plt.subplot(4, 2, i+1)\n","    plt.title(new_column_name)\n","    plt.plot(amplitude)\n","    plt.tight_layout()\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:43:39.505922Z","iopub.status.busy":"2024-02-20T08:43:39.505008Z","iopub.status.idle":"2024-02-20T08:43:51.443085Z","shell.execute_reply":"2024-02-20T08:43:51.442113Z","shell.execute_reply.started":"2024-02-20T08:43:39.505887Z"},"trusted":true},"outputs":[],"source":["# 4. spectral centroid column for all the audio files\n","spectral_centroid_list = []\n","plt.figure(figsize=(15, 10))\n","\n","for i in range(8):\n","    column_name = f\"recording_{i+1}\"\n","    new_column_name = f\"spectral_centroid_{i+1}\"\n","    spectral_centroid = []\n","    for aud in train_df[column_name]:\n","        # file_path = train_dir_path + audio_file + \".wav\"\n","        # aud, sr = librosa.load(file_path, sr=None)\n","        spectral_centroid.append(librosa.feature.spectral_centroid(y=np.array(aud), sr=sr).mean())\n","    spectral_centroid_list.append(spectral_centroid)\n","    train_df[new_column_name] = spectral_centroid\n","    plt.subplot(4, 2, i+1)\n","    plt.title(new_column_name)\n","    plt.plot(spectral_centroid)\n","    plt.tight_layout()\n","plt.show()\n","\n","\n","spectral_centroid_list = []\n","plt.figure(figsize=(15, 10))\n","\n","for i in range(8):\n","    column_name = f\"recording_{i+1}\"\n","    new_column_name = f\"spectral_centroid_{i+1}\"\n","    spectral_centroid = []\n","    for audio_file in test_df[column_name]:\n","        file_path = test_dir_path + audio_file + \".wav\"\n","        aud, sr = librosa.load(file_path, sr=None)\n","        spectral_centroid.append(librosa.feature.spectral_centroid(y=aud, sr=sr).mean())\n","    spectral_centroid_list.append(spectral_centroid)\n","    test_df[new_column_name] = spectral_centroid\n","    plt.subplot(4, 2, i+1)\n","    plt.title(new_column_name)\n","    plt.plot(spectral_centroid)\n","    plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:43:51.445035Z","iopub.status.busy":"2024-02-20T08:43:51.444348Z","iopub.status.idle":"2024-02-20T08:44:03.813342Z","shell.execute_reply":"2024-02-20T08:44:03.812477Z","shell.execute_reply.started":"2024-02-20T08:43:51.445000Z"},"trusted":true},"outputs":[],"source":["# 5. spectral roll-off column for all the audio files\n","spectral_rolloff_list = []\n","plt.figure(figsize=(15, 10))\n","\n","for i in range(8):\n","    column_name = f\"recording_{i+1}\"\n","    new_column_name = f\"spectral_rolloff_{i+1}\"\n","    spectral_rolloff = []\n","    for aud in train_df[column_name]:\n","        # file_path = train_dir_path + audio_file + \".wav\"\n","        # aud, sr = librosa.load(file_path, sr=None)\n","        spectral_rolloff.append(librosa.feature.spectral_rolloff(y=np.array(aud), sr=sr).mean())\n","    spectral_rolloff_list.append(spectral_rolloff)\n","    train_df[new_column_name] = spectral_rolloff\n","    plt.subplot(4, 2, i+1)\n","    plt.title(new_column_name)\n","    plt.plot(spectral_rolloff)\n","    plt.tight_layout()\n","plt.show()\n","\n","\n","spectral_rolloff_list = []\n","plt.figure(figsize=(15, 10))\n","\n","for i in range(8):\n","    column_name = f\"recording_{i+1}\"\n","    new_column_name = f\"spectral_rolloff_{i+1}\"\n","    spectral_rolloff = []\n","    for audio_file in test_df[column_name]:\n","        file_path = test_dir_path + audio_file + \".wav\"\n","        aud, sr = librosa.load(file_path, sr=None)\n","        spectral_rolloff.append(librosa.feature.spectral_rolloff(y=aud, sr=sr).mean())\n","    spectral_rolloff_list.append(spectral_rolloff)\n","    test_df[new_column_name] = spectral_rolloff\n","    plt.subplot(4, 2, i+1)\n","    plt.title(new_column_name)\n","    plt.plot(spectral_rolloff)\n","    plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:44:03.815268Z","iopub.status.busy":"2024-02-20T08:44:03.814654Z","iopub.status.idle":"2024-02-20T08:44:42.210697Z","shell.execute_reply":"2024-02-20T08:44:42.209645Z","shell.execute_reply.started":"2024-02-20T08:44:03.815235Z"},"trusted":true},"outputs":[],"source":["# 6. mel-frequency cepstral coefficients (MFCCs) column for all the audio files\n","mfcc_list = []\n","plt.figure(figsize=(15, 10))\n","\n","for i in range(8):\n","    column_name = f\"recording_{i+1}\"\n","    new_column_name = f\"mfcc_{i+1}\"\n","    mfcc = []\n","    for aud in train_df[column_name]:\n","        # file_path = train_dir_path + audio_file + \".wav\"\n","        # aud, sr = librosa.load(file_path, sr=None)\n","        mfcc.append(librosa.feature.mfcc(y=np.array(aud), sr=sr, n_mfcc=13).mean())\n","    mfcc_list.append(mfcc)\n","    train_df[new_column_name] = mfcc\n","    plt.subplot(4, 2, i+1)\n","    plt.title(new_column_name)\n","    plt.plot(mfcc)\n","    plt.tight_layout()\n","plt.show()\n","\n","mfcc_list = []\n","plt.figure(figsize=(15, 10))\n","\n","for i in range(8):\n","    column_name = f\"recording_{i+1}\"\n","    new_column_name = f\"mfcc_{i+1}\"\n","    mfcc = []\n","    for audio_file in test_df[column_name]:\n","        file_path = test_dir_path + audio_file + \".wav\"\n","        aud, sr = librosa.load(file_path, sr=None)\n","        mfcc.append(librosa.feature.mfcc(y=aud, sr=sr, n_mfcc=13).mean())\n","    mfcc_list.append(mfcc)\n","    test_df[new_column_name] = mfcc\n","    plt.subplot(4, 2, i+1)\n","    plt.title(new_column_name)\n","    plt.plot(mfcc)\n","    plt.tight_layout()\n","plt.show()\n","\n","print(train_df.head())\n","print(test_df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:44:42.212994Z","iopub.status.busy":"2024-02-20T08:44:42.212539Z","iopub.status.idle":"2024-02-20T08:45:20.451179Z","shell.execute_reply":"2024-02-20T08:45:20.448828Z","shell.execute_reply.started":"2024-02-20T08:44:42.212952Z"},"trusted":true},"outputs":[],"source":["# 7. chroma feature column for all the audio files\n","chroma_stft_list = []\n","plt.figure(figsize=(15, 10))\n","\n","for i in range(8):\n","    column_name = f\"recording_{i+1}\"\n","    new_column_name = f\"chroma_stft_{i+1}\"\n","    chroma_stft = []\n","    for aud in train_df[column_name]:\n","        # file_path = train_dir_path + audio_file + \".wav\"\n","        # aud, sr = librosa.load(file_path, sr=None)\n","        chroma_stft.append(librosa.feature.chroma_stft(y=np.array(aud), sr=sr).mean())\n","    chroma_stft_list.append(chroma_stft)\n","    train_df[new_column_name] = chroma_stft\n","    plt.subplot(4, 2, i+1)\n","    plt.title(new_column_name)\n","    plt.plot(chroma_stft)\n","    plt.tight_layout()\n","plt.show()\n","\n","chroma_stft_list = []\n","plt.figure(figsize=(15, 10))\n","\n","for i in range(8):\n","    column_name = f\"recording_{i+1}\"\n","    new_column_name = f\"chroma_stft_{i+1}\"\n","    chroma_stft = []\n","    for audio_file in test_df[column_name]:\n","        file_path = test_dir_path + audio_file + \".wav\"\n","        aud, sr = librosa.load(file_path, sr=None)\n","        chroma_stft.append(librosa.feature.chroma_stft(y=aud, sr=sr).mean())\n","    chroma_stft_list.append(chroma_stft)\n","    test_df[new_column_name] = chroma_stft\n","    plt.subplot(4, 2, i+1)\n","    plt.title(new_column_name)\n","    plt.plot(chroma_stft)\n","    plt.tight_layout()\n","plt.show()\n","\n","print(train_df.head())\n","print(test_df.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:45:20.453358Z","iopub.status.busy":"2024-02-20T08:45:20.452959Z","iopub.status.idle":"2024-02-20T08:45:20.465899Z","shell.execute_reply":"2024-02-20T08:45:20.464814Z","shell.execute_reply.started":"2024-02-20T08:45:20.453320Z"},"trusted":true},"outputs":[],"source":["# drop all the recording name columns from the train and test data\n","train_df.drop(columns=[f\"recording_{i+1}\" for i in range(8)], inplace=True)\n","test_df.drop(columns=[f\"recording_{i+1}\" for i in range(8)], inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:45:20.469690Z","iopub.status.busy":"2024-02-20T08:45:20.467670Z","iopub.status.idle":"2024-02-20T08:45:20.484431Z","shell.execute_reply":"2024-02-20T08:45:20.483266Z","shell.execute_reply.started":"2024-02-20T08:45:20.469650Z"},"trusted":true},"outputs":[],"source":["# split the train data into features and target\n","train_ids = train_df[\"patient_id\"]\n","test_ids = test_df[\"patient_id\"]\n","y = train_df[[\"AS\", \"AR\", \"MR\", \"MS\", \"N\"]]\n","train_df = train_df.drop(columns=[\"patient_id\", \"AS\", \"AR\", \"MR\", \"MS\", \"N\"])\n","X = train_df\n","test_df = test_df.drop(columns=[\"patient_id\"])\n","Y = test_df\n","print(X.shape)\n","print(y.shape)\n","print(Y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:45:20.486704Z","iopub.status.busy":"2024-02-20T08:45:20.486279Z","iopub.status.idle":"2024-02-20T08:45:20.494909Z","shell.execute_reply":"2024-02-20T08:45:20.493885Z","shell.execute_reply.started":"2024-02-20T08:45:20.486668Z"},"trusted":true},"outputs":[],"source":["# save the train and test data to a temporary csv file\n","filename = \"segmented_zero_centroid_rolloff_mfcc_chroma.csv\"\n","train_filename = \"segmented_zero_energy_amplitude_centroid_rolloff_mfcc_chroma_train.csv\"\n","test_filename = \"segmented_zero_energy_amplitude_centroid_rolloff_mfcc_chroma_test.csv\"\n","# X.to_csv(train_filename, index=False)\n","# Y.to_csv(test_filename, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:45:42.752058Z","iopub.status.busy":"2024-02-20T08:45:42.751630Z","iopub.status.idle":"2024-02-20T08:45:42.758132Z","shell.execute_reply":"2024-02-20T08:45:42.756607Z","shell.execute_reply.started":"2024-02-20T08:45:42.752028Z"},"trusted":true},"outputs":[],"source":["# # load the train and test data\n","# X = pd.read_csv(train_filename)\n","# Y = pd.read_csv(test_filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:46:41.338866Z","iopub.status.busy":"2024-02-20T08:46:41.338342Z","iopub.status.idle":"2024-02-20T08:46:41.382829Z","shell.execute_reply":"2024-02-20T08:46:41.381103Z","shell.execute_reply.started":"2024-02-20T08:46:41.338824Z"},"trusted":true},"outputs":[],"source":["X"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:47:19.868666Z","iopub.status.busy":"2024-02-20T08:47:19.867484Z","iopub.status.idle":"2024-02-20T08:47:19.883031Z","shell.execute_reply":"2024-02-20T08:47:19.881691Z","shell.execute_reply.started":"2024-02-20T08:47:19.868625Z"},"trusted":true},"outputs":[],"source":["y"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:47:26.438899Z","iopub.status.busy":"2024-02-20T08:47:26.438431Z","iopub.status.idle":"2024-02-20T08:47:26.444797Z","shell.execute_reply":"2024-02-20T08:47:26.443474Z","shell.execute_reply.started":"2024-02-20T08:47:26.438865Z"},"trusted":true},"outputs":[],"source":["# make the y values into integer\n","y = y.astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:47:35.996634Z","iopub.status.busy":"2024-02-20T08:47:35.996180Z","iopub.status.idle":"2024-02-20T08:47:36.002154Z","shell.execute_reply":"2024-02-20T08:47:36.000848Z","shell.execute_reply.started":"2024-02-20T08:47:35.996599Z"},"trusted":true},"outputs":[],"source":["# # plot the target label imbalance\n","# plt.figure(figsize=(15, 10))\n","\n","# # check for target label imbalance\n","# for cls in y.columns:\n","#     print(f\"{cls}: {y[cls].value_counts()}\")\n","#     plt.subplot(2, 3, list(y.columns).index(cls)+1)\n","#     sns.countplot(x=cls, data=y)\n","#     plt.title(cls)\n","#     plt.tight_layout()\n","# plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:47:36.399769Z","iopub.status.busy":"2024-02-20T08:47:36.399034Z","iopub.status.idle":"2024-02-20T08:47:36.405063Z","shell.execute_reply":"2024-02-20T08:47:36.403951Z","shell.execute_reply.started":"2024-02-20T08:47:36.399735Z"},"trusted":true},"outputs":[],"source":["# # normalize the data\n","# from sklearn.preprocessing import StandardScaler\n","# scaler = StandardScaler()\n","# X = scaler.fit_transform(X)\n","# Y = scaler.transform(Y)\n","\n","\n","# # principal component analysis and dimensionality reduction\n","# from sklearn.decomposition import PCA\n","# pca = PCA(n_components=2)\n","# X_pca = pca.fit_transform(X)\n","# Y_pca = pca.transform(Y)\n","\n","# # plot the pca\n","# plt.figure(figsize=(15, 10))\n","# plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y[\"AS\"], cmap=\"viridis\", label=\"AS\")\n","# plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y[\"AR\"], cmap=\"viridis\", label=\"AR\")\n","# plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y[\"MR\"], cmap=\"viridis\", label=\"MR\")\n","# plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y[\"MS\"], cmap=\"viridis\", label=\"MS\")\n","# plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y[\"N\"], cmap=\"viridis\", label=\"N\")\n","# plt.xlabel(\"Principal Component 1\")\n","# plt.ylabel(\"Principal Component 2\")\n","# plt.legend()\n","# plt.tight_layout()\n","# plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["# Decision Tree Classifier With Feature Importance"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:47:42.428970Z","iopub.status.busy":"2024-02-20T08:47:42.428566Z","iopub.status.idle":"2024-02-20T08:47:56.055753Z","shell.execute_reply":"2024-02-20T08:47:56.054462Z","shell.execute_reply.started":"2024-02-20T08:47:42.428942Z"},"trusted":true},"outputs":[],"source":["# normalize the data\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","Y = scaler.transform(Y)\n","\n","\n","# use a leave-one-out cross-validation\n","from sklearn.model_selection import LeaveOneOut\n","from sklearn.metrics import f1_score\n","loo = LeaveOneOut()\n","\n","# create a decision tree classifier\n","dt = DecisionTreeClassifier(random_state=42)\n","\n","# create a list to store the predictions\n","accuracies = []\n","# iterate over the leave-one-out cross-validation and plot the accuracy\n","for train_index, test_index in loo.split(X):\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","    print(X_train.shape)\n","    print(y_train.shape)\n","    dt.fit(X_train, y_train)\n","    y_pred = dt.predict(X_test)\n","    print(f\"true label: {y_test.values[0]}\")\n","    print(f\"predicted label: {y_pred[0]}\")\n","    # calculate the f1 score\n","    accuracies.append(f1_score(y_test, y_pred, average=\"macro\"))\n","    print(f\"f1 score: {f1_score(y_test, y_pred, average='macro')}\")\n","print(f\"mean f1 score: {np.mean(accuracies)}\")\n","# plot the f1 scores\n","plt.plot(accuracies)\n","plt.xlabel(\"Iteration\")\n","plt.ylabel(\"F1 Score\")\n","plt.title(\"F1 Score vs Iteration\")\n","\n","# fit the model on the entire train data\n","dt.fit(X, y)\n","# make predictions on the test data\n","test_preds = dt.predict(Y)\n","# create a submission dataframe\n","submission = pd.DataFrame(test_preds, columns=[\"AS\", \"AR\", \"MR\", \"MS\", \"N\"])\n","submission[\"patient_id\"] = test_ids\n","submission = submission[[\"patient_id\", \"AS\", \"AR\", \"MR\", \"MS\", \"N\"]]\n","# check the submission\n","print(submission.head())\n","\n","\n","# feature importance of the decision tree\n","feature_importance = dt.feature_importances_\n","# print all the feature importance and their corresponding column names\n","print(feature_importance)\n","print(train_df.columns)\n","\n","column_names = train_df.columns\n","\n","# plot the feature importance\n","plt.figure(figsize=(15, 10))\n","sns.barplot(x=feature_importance, y=column_names)\n","plt.title(\"Feature Importance\")\n","plt.show()\n","\n","# select the top 35 features from the sorted values of the feature importance\n","top_35_features = column_names[np.argsort(feature_importance)[::-1][:35]]\n","# print the top 35 features and their importance values\n","print(top_35_features)\n","print(feature_importance[np.argsort(feature_importance)[::-1][:35]])\n","\n","# only keep these features in the train and test data\n","X = train_df[top_35_features]\n","Y = test_df[top_35_features]\n","\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","Y = scaler.transform(Y)\n","\n","# use a leave-one-out cross-validation\n","from sklearn.model_selection import LeaveOneOut\n","from sklearn.metrics import f1_score\n","loo = LeaveOneOut()\n","\n","# create a decision tree classifier\n","dt = DecisionTreeClassifier(random_state=42)\n","\n","# create a list to store the predictions\n","accuracies = []\n","# iterate over the leave-one-out cross-validation and plot the accuracy\n","for train_index, test_index in loo.split(X):\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","    dt.fit(X_train, y_train)\n","    y_pred = dt.predict(X_test)\n","    print(f\"true label: {y_test.values[0]}\")\n","    print(f\"predicted label: {y_pred[0]}\")\n","    # calculate the f1 score\n","    accuracies.append(f1_score(y_test, y_pred, average=\"macro\"))\n","    print(f\"f1 score: {f1_score(y_test, y_pred, average='macro')}\")\n","print(f\"mean f1 score: {np.mean(accuracies)}\")\n","# plot the f1 scores\n","plt.plot(accuracies)\n","plt.xlabel(\"Iteration\")\n","plt.ylabel(\"F1 Score\")\n","plt.title(\"F1 Score vs Iteration\")\n","\n","# fit the model on the entire train data\n","dt.fit(X, y)\n","# make predictions on the test data\n","test_preds = dt.predict(Y)\n","# create a submission dataframe\n","submission = pd.DataFrame(test_preds, columns=[\"AS\", \"AR\", \"MR\", \"MS\", \"N\"])\n","submission[\"patient_id\"] = test_ids\n","submission = submission[[\"patient_id\", \"AS\", \"AR\", \"MR\", \"MS\", \"N\"]]\n","# check the submission\n","print(submission.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-02-20T08:48:15.974662Z","iopub.status.busy":"2024-02-20T08:48:15.974169Z","iopub.status.idle":"2024-02-20T08:48:15.982759Z","shell.execute_reply":"2024-02-20T08:48:15.981617Z","shell.execute_reply.started":"2024-02-20T08:48:15.974626Z"},"trusted":true},"outputs":[],"source":["submission.to_csv(\"submission_dt_35_segmented_7_feat.csv\", index=False) # 0.3853"]},{"cell_type":"markdown","metadata":{},"source":["# XGBoost"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-20T08:45:21.170839Z","iopub.status.idle":"2024-02-20T08:45:21.171374Z","shell.execute_reply":"2024-02-20T08:45:21.171123Z","shell.execute_reply.started":"2024-02-20T08:45:21.171100Z"},"trusted":true},"outputs":[],"source":["# # use a leave-one-out cross-validation\n","# from sklearn.model_selection import LeaveOneOut\n","# from sklearn.metrics import f1_score\n","# loo = LeaveOneOut()\n","\n","# # normalize the data\n","# from sklearn.preprocessing import StandardScaler\n","# scaler = StandardScaler()\n","# X = scaler.fit_transform(X)\n","# Y = scaler.transform(Y)\n","\n","# # try xtreme gradient boosting\n","# from xgboost import XGBClassifier\n","# xgb = XGBClassifier(random_state=42)\n","\n","# # create a list to store the predictions\n","# accuracies = []\n","# # iterate over the leave-one-out cross-validation and plot the accuracy\n","# for train_index, test_index in loo.split(X):\n","#     X_train, X_test = X[train_index], X[test_index]\n","#     y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","#     xgb.fit(X_train, y_train)\n","#     y_pred = xgb.predict(X_test)\n","#     print(f\"true label: {y_test.values[0]}\")\n","#     print(f\"predicted label: {y_pred[0]}\")\n","#     # calculate the f1 score\n","#     accuracies.append(f1_score(y_test, y_pred, average=\"macro\"))\n","#     print(f\"f1 score: {f1_score(y_test, y_pred, average='macro')}\")\n","\n","# print(f\"mean f1 score: {np.mean(accuracies)}\")\n","# # plot the f1 scores\n","# plt.plot(accuracies)\n","# plt.xlabel(\"Iteration\")\n","# plt.ylabel(\"F1 Score\")\n","# plt.title(\"F1 Score vs Iteration\")\n","# plt.show()\n","\n","# # feature importance of the xgboot classifier\n","# feature_importance = xgb.feature_importances_\n","# # print all the feature importance and their corresponding column names\n","# print(feature_importance)\n","# print(train_df.columns)\n","\n","# column_names = train_df.columns\n","\n","# # plot the feature importance\n","# plt.figure(figsize=(15, 10))\n","# sns.barplot(x=feature_importance, y=column_names)\n","# plt.title(\"Feature Importance\")\n","\n","# # select the top 30 features from the sorted values of the feature importance\n","# top_30_features = column_names[np.argsort(feature_importance)[::-1][:30]]\n","# # print the top 30 features and their importance values\n","# print(top_30_features)\n","# print(feature_importance[np.argsort(feature_importance)[::-1][:30]])\n","\n","# # only keep these features in the train and test data\n","# X = train_df[top_30_features]\n","# Y = test_df[top_30_features]\n","\n","# from sklearn.preprocessing import StandardScaler\n","# scaler = StandardScaler()\n","# X = scaler.fit_transform(X)\n","# Y = scaler.transform(Y)\n","\n","# xgb = XGBClassifier(random_state=42)\n","\n","# # use a leave-one-out cross-validation\n","# from sklearn.model_selection import LeaveOneOut\n","# from sklearn.metrics import f1_score\n","# loo = LeaveOneOut()\n","\n","# # create a list to store the predictions\n","# accuracies = []\n","# # iterate over the leave-one-out cross-validation and plot the accuracy\n","# for train_index, test_index in loo.split(X):\n","#     X_train, X_test = X[train_index], X[test_index]\n","#     y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","#     xgb.fit(X_train, y_train)\n","#     y_pred = xgb.predict(X_test)\n","#     print(f\"true label: {y_test.values[0]}\")\n","#     print(f\"predicted label: {y_pred[0]}\")\n","#     # calculate the f1 score\n","#     accuracies.append(f1_score(y_test, y_pred, average=\"macro\"))\n","#     print(f\"f1 score: {f1_score(y_test, y_pred, average='macro')}\")\n","\n","# print(f\"mean f1 score: {np.mean(accuracies)}\")\n","# # plot the f1 scores\n","# plt.plot(accuracies)\n","# plt.xlabel(\"Iteration\")\n","# plt.ylabel(\"F1 Score\")\n","# plt.title(\"F1 Score vs Iteration\")\n","# plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-20T08:45:21.173987Z","iopub.status.idle":"2024-02-20T08:45:21.174612Z","shell.execute_reply":"2024-02-20T08:45:21.174328Z","shell.execute_reply.started":"2024-02-20T08:45:21.174302Z"},"trusted":true},"outputs":[],"source":["# # fit the model on the entire train data\n","# xgb.fit(X, y)\n","# # make predictions on the test data\n","# test_preds = xgb.predict(Y)\n","# # create a submission dataframe\n","# submission = pd.DataFrame(test_preds, columns=[\"AS\", \"AR\", \"MR\", \"MS\", \"N\"])\n","# submission[\"patient_id\"] = test_ids\n","# submission = submission[[\"patient_id\", \"AS\", \"AR\", \"MR\", \"MS\", \"N\"]]\n","# # check the submission\n","# print(submission.head())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-20T08:45:21.175928Z","iopub.status.idle":"2024-02-20T08:45:21.176821Z","shell.execute_reply":"2024-02-20T08:45:21.176556Z","shell.execute_reply.started":"2024-02-20T08:45:21.176533Z"},"trusted":true},"outputs":[],"source":["# submission.to_csv(\"submission_xgb_30_segmented_7_feat.csv\", index=False) # 0.3984"]},{"cell_type":"markdown","metadata":{},"source":["# Random Forest"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-20T08:45:21.178783Z","iopub.status.idle":"2024-02-20T08:45:21.179303Z","shell.execute_reply":"2024-02-20T08:45:21.179054Z","shell.execute_reply.started":"2024-02-20T08:45:21.179031Z"},"trusted":true},"outputs":[],"source":["# # try random forest\n","\n","# # normalize the data\n","# from sklearn.preprocessing import StandardScaler\n","# from sklearn.metrics import f1_score\n","# scaler = StandardScaler()\n","# X = scaler.fit_transform(X)\n","# Y = scaler.transform(Y)\n","\n","# rf = RandomForestClassifier(random_state=42, n_estimators=100)\n","\n","# # use a leave-one-out cross-validation\n","# from sklearn.model_selection import LeaveOneOut\n","# loo = LeaveOneOut()\n","\n","# # create a list to store the predictions\n","# accuracies = []\n","# # iterate over the leave-one-out cross-validation and plot the accuracy\n","# for train_index, test_index in loo.split(X):\n","#     X_train, X_test = X[train_index], X[test_index]\n","#     y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","#     rf.fit(X_train, y_train)\n","#     y_pred = rf.predict(X_test)\n","#     print(f\"true label: {y_test.values[0]}\")\n","#     print(f\"predicted label: {y_pred[0]}\")\n","#     # calculate the f1 score\n","#     accuracies.append(f1_score(y_test, y_pred, average=\"macro\"))\n","#     print(f\"f1 score: {f1_score(y_test, y_pred, average='macro')}\")\n","\n","# print(f\"mean f1 score: {np.mean(accuracies)}\")\n","# # plot the f1 scores\n","# plt.plot(accuracies)\n","# plt.xlabel(\"Iteration\")\n","# plt.ylabel(\"F1 Score\")\n","# plt.title(\"F1 Score vs Iteration\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-20T08:45:21.181017Z","iopub.status.idle":"2024-02-20T08:45:21.181548Z","shell.execute_reply":"2024-02-20T08:45:21.181286Z","shell.execute_reply.started":"2024-02-20T08:45:21.181264Z"},"trusted":true},"outputs":[],"source":["# # fit the model on the entire train data\n","# rf.fit(X, y)\n","# # make predictions on the test data\n","# test_preds = rf.predict(Y)\n","# # create a submission dataframe\n","# submission = pd.DataFrame(test_preds, columns=[\"AS\", \"AR\", \"MR\", \"MS\", \"N\"])\n","# submission[\"patient_id\"] = test_ids\n","# submission = submission[[\"patient_id\", \"AS\", \"AR\", \"MR\", \"MS\", \"N\"]]\n","# # check the submission\n","# print(submission.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-20T08:45:21.183346Z","iopub.status.idle":"2024-02-20T08:45:21.184248Z","shell.execute_reply":"2024-02-20T08:45:21.183984Z","shell.execute_reply.started":"2024-02-20T08:45:21.183958Z"},"trusted":true},"outputs":[],"source":["# # save the submission\n","# submission.to_csv(\"submission_rf_7_feat.csv\", index=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Multi Layer Perceptron"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-20T08:45:21.185960Z","iopub.status.idle":"2024-02-20T08:45:21.186496Z","shell.execute_reply":"2024-02-20T08:45:21.186231Z","shell.execute_reply.started":"2024-02-20T08:45:21.186208Z"},"trusted":true},"outputs":[],"source":["# # apply complex multi-layer perceptron\n","# from sklearn.neural_network import MLPClassifier\n","# from sklearn.preprocessing import StandardScaler\n","# from sklearn.model_selection import LeaveOneOut\n","# from sklearn.metrics import f1_score\n","\n","# # normalize the data\n","# scaler = StandardScaler()\n","# X = scaler.fit_transform(X)\n","# Y = scaler.transform(Y)\n","\n","# # create a multi-layer perceptron classifier\n","# mlp = MLPClassifier(random_state=42, max_iter=100000)\n","\n","# # use a leave-one-out cross-validation\n","# loo = LeaveOneOut()\n","\n","# # create a list to store the predictions\n","# accuracies = []\n","# # iterate over the leave-one-out cross-validation and plot the accuracy\n","\n","# for train_index, test_index in loo.split(X):\n","#     X_train, X_test = X[train_index], X[test_index]\n","#     y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","#     mlp.fit(X_train, y_train)\n","#     y_pred = mlp.predict(X_test)\n","#     print(f\"true label: {y_test.values[0]}\")\n","#     print(f\"predicted label: {y_pred[0]}\")\n","#     # calculate the f1 score\n","#     accuracies.append(f1_score(y_test, y_pred, average=\"macro\"))\n","#     print(f\"f1 score: {f1_score(y_test, y_pred, average='macro')}\")\n","\n","# print(f\"mean f1 score: {np.mean(accuracies)}\")\n","# # plot the f1 scores\n","# plt.plot(accuracies)\n","# plt.xlabel(\"Iteration\")\n","# plt.ylabel(\"F1 Score\")\n","# plt.title(\"F1 Score vs Iteration\")\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-20T08:45:21.188460Z","iopub.status.idle":"2024-02-20T08:45:21.188984Z","shell.execute_reply":"2024-02-20T08:45:21.188736Z","shell.execute_reply.started":"2024-02-20T08:45:21.188713Z"},"trusted":true},"outputs":[],"source":["# # fit the model on the entire train data\n","# mlp.fit(X, y)\n","# # make predictions on the test data\n","# test_preds = mlp.predict(Y)\n","# # create a submission dataframe\n","# submission = pd.DataFrame(test_preds, columns=[\"AS\", \"AR\", \"MR\", \"MS\", \"N\"])\n","# submission[\"patient_id\"] = test_ids\n","# submission = submission[[\"patient_id\", \"AS\", \"AR\", \"MR\", \"MS\", \"N\"]]\n","# # check the submission\n","# print(submission.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-20T08:45:21.191636Z","iopub.status.idle":"2024-02-20T08:45:21.192156Z","shell.execute_reply":"2024-02-20T08:45:21.191912Z","shell.execute_reply.started":"2024-02-20T08:45:21.191889Z"},"trusted":true},"outputs":[],"source":["# save the submission\n","# submission.to_csv(\"submission_mlp_7_feat.csv\", index=False) # 0.3984"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-20T08:45:21.194189Z","iopub.status.idle":"2024-02-20T08:45:21.194741Z","shell.execute_reply":"2024-02-20T08:45:21.194478Z","shell.execute_reply.started":"2024-02-20T08:45:21.194456Z"},"trusted":true},"outputs":[],"source":["# # handle the data imbalance using MLSMOTE for multi label classification\n","# from collections import Counter\n","# from imblearn.over_sampling import SMOTE\n","# from imblearn.over_sampling import SMOTENC\n","\n","# # check the distribution of the target classes\n","# print(Counter(y[\"AS\"]))\n","# print(Counter(y[\"AR\"]))\n","# print(Counter(y[\"MR\"]))\n","# print(Counter(y[\"MS\"]))\n","# print(Counter(y[\"N\"]))\n","\n","# X_trains = []\n","# y_trains = []\n","\n","# # X -> train features, y -> train target, Y -> test features\n","\n","# for cls in target_classes:\n","#     smote = SMOTENC(random_state=42, categorical_features=[0, 1, 2, 3, 4])\n","#     X_train_resampled, y_train_resampled = smote.fit_resample(X, y[cls])\n","#     print(f\"resampled shape for {cls}: {X_train_resampled.shape}\")\n","#     print(f\"resampled target shape for {cls}: {y_train_resampled.shape}\")\n","#     X_trains.append(X_train_resampled)\n","#     y_trains.append(y_train_resampled)\n","\n","\n","# # Now X_train_resampled and y_train_resampled contain the oversampled data for each label\n","\n","# # check the distribution of the target classes\n","# print(Counter(y_trains[0]))\n","# print(Counter(y_trains[1]))\n","# print(Counter(y_trains[2]))\n","# print(Counter(y_trains[3]))\n","# print(Counter(y_trains[4]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-02-20T08:45:21.196751Z","iopub.status.idle":"2024-02-20T08:45:21.197263Z","shell.execute_reply":"2024-02-20T08:45:21.197021Z","shell.execute_reply.started":"2024-02-20T08:45:21.196999Z"},"trusted":true},"outputs":[],"source":["# # concatanate the train features and train labels csv into one\n","# new_csv = pd.concat([X, y], axis=1)\n","# # save the new csv\n","# new_csv.to_csv(\"train_features_labels.csv\", index=False)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":7683890,"sourceId":70055,"sourceType":"competition"}],"dockerImageVersionId":30646,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
